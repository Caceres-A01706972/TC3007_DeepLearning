# Proyecto de Reconocimiento de Emociones mediante Machine Learning

## Descripci칩n del Proyecto 游

Las emociones desempe침an un papel crucial en la interacci칩n humana, y el reconocimiento de emociones en im치genes se ha vuelto esencial para diversas aplicaciones. Este proyecto aborda el reconocimiento de emociones en distintas im치genes mediante un modelo de Deep Learning. El objetivo principal es dise침ar y entrenar una red neuronal profunda capaz de diferenciar entre las emociones de **enojo**, **felicidad** y **tristeza**, modelando as칤 la expresi칩n emocional humana a trav칠s de im치genes.

## Conjunto de Datos 游늵

El conjunto de datos utilizado en este proyecto es esencial para la correcta formaci칩n y evaluaci칩n del modelo de reconocimiento de emociones. Est치 organizado jer치rquicamente en una carpeta principal denominada **"data"**, que contiene tres subcarpetas representando cada emoci칩n espec칤fica: **"Happy"**, **"Sad"** y **"Angry"**. Cada imagen est치 etiquetada correctamente gracias a su path con la emoci칩n correspondiente.

- **Happy:** Im치genes que representan expresiones asociadas con la felicidad, capturando momentos felices con gestos evidentes de alegr칤a.
- **Sad:** Im치genes que reflejan expresiones vinculadas a la tristeza, con manifestaciones m치s pronunciadas de pensamiento y colores grises y oscuros.
- **Angry:** Im치genes que representan emociones de enojo, con colores fuertes y gestos intensos.

El conjunto de datos se distribuye de la siguiente manera:

- **Train:** 90% de los datos.
- **Validation:** 10% de los datos.
- **Test:** 20% del conjunto de entrenamiento.

Las im치genes tienen un formato de 224x224 p칤xeles con 3 canales (RGB) para garantizar una entrada coherente al modelo.

## Arquitectura del Modelo 游

La arquitectura inicial del modelo se bas칩 en la red neuronal preentrenada **VGG16** mediante **Transfer Learning** con ImageNet. Se cre칩 un modelo secuencial para agregar capas adicionales adaptadas al problema espec칤fico. La arquitectura consta de:

1. Capas Convolutivas de **VGG16**.
2. Capa **Flatten** para convertir la salida tridimensional en un vector unidimensional.
3. Capa Densa con **128 neuronas** y activaci칩n **ReLU**.
4. Capa de **Dropout del 40%** para prevenir el sobreajuste.
5. Capa Densa de salida con **3 neuronas** y activaci칩n **softmax** para las predicciones de las 3 clases.

## Predicciones y Despliegue 游

El modelo entrenado se guarda para realizar predicciones. El archivo **"userTest.ipynb"** proporciona una manera interactiva de hacer predicciones con el modelo en nuevas im치genes. Adem치s, el archivo **"app.py"** crea un servidor en **Flask** que despliega una interfaz gr치fica para facilitar la carga de im치genes y realizar predicciones.

춰Gracias por explorar nuestro proyecto de reconocimiento de emociones mediante Machine Learning! 游깯

